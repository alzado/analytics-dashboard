import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import data_processing as dp
import visualizations as viz

# Page configuration
st.set_page_config(
    page_title="Search Analytics Dashboard",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main > div {
        padding-top: 2rem;
    }
    .stMetric {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #1f77b4;
    }
    </style>
""", unsafe_allow_html=True)

# Title
st.title("üîç Search Analytics Dashboard")
st.caption("üíµ All monetary values displayed in CLP (Chilean Pesos)")

# Data Source Configuration
st.sidebar.header("üìä Data Source")
data_source = st.sidebar.radio(
    "Choose data source:",
    ["CSV File", "BigQuery"],
    key='data_source'
)

# Load data based on source
@st.cache_data
def load_and_process_data(source, bq_params=None):
    """Load and process the search data (CSV mode only)."""
    # CSV mode: load full data as before
    df = dp.load_data('bquxjob_6268ac9c_19a163e3339.csv')
    df = dp.calculate_metrics(df)
    return df


def initialize_bigquery_connection(bq_params):
    """
    Initialize BigQuery connection and store parameters in session state.
    For on-demand query mode.
    """
    # Store connection parameters
    st.session_state.bq_params = bq_params
    # Note: Don't set data_source here - it's bound to a widget key

    # Initialize BigQuery client
    st.session_state.bq_client = dp.get_bigquery_client(
        project_id=bq_params['project_id'],
        credentials_path=bq_params.get('credentials_path')
    )

    # Query min/max dates from the table for date range widget
    try:
        date_query = f"""
            SELECT
                MIN(date) as min_date,
                MAX(date) as max_date
            FROM `{bq_params['table']}`
        """
        date_df = dp.execute_cached_query(st.session_state.bq_client, date_query, ttl=3600)

        st.session_state.bq_min_date = pd.to_datetime(date_df['min_date'].iloc[0])
        st.session_state.bq_max_date = pd.to_datetime(date_df['max_date'].iloc[0])
    except Exception as e:
        st.error(f"Error querying date range: {str(e)}")
        # Fallback to defaults
        from datetime import datetime, timedelta
        st.session_state.bq_min_date = datetime.now() - timedelta(days=365)
        st.session_state.bq_max_date = datetime.now()

    st.session_state.bq_initialized = True

# BigQuery configuration
bq_params = None
if data_source == "BigQuery":
    st.sidebar.subheader("BigQuery Settings")

    # Default to the provided table
    default_table = "tc-sc-bi-bigdata-fcom-dev.sandbox_lmellado.query_intent_classification"

    bq_project = st.sidebar.text_input(
        "Project ID",
        value="tc-sc-bi-bigdata-fcom-dev",
        help="GCP project ID"
    )

    bq_table = st.sidebar.text_input(
        "Table",
        value=default_table,
        help="Full table path (project.dataset.table or dataset.table)"
    )

    bq_creds = st.sidebar.text_input(
        "Credentials Path (optional)",
        value="",
        help="Path to service account JSON file. Leave empty to use default credentials."
    )

    st.sidebar.markdown("---")
    st.sidebar.subheader("‚ö° Performance Filters")
    st.sidebar.caption("Apply filters BEFORE loading data (recommended for large tables)")

    # Date range filter (CRITICAL for 22M records!)
    from datetime import date, timedelta

    col1, col2 = st.sidebar.columns(2)
    with col1:
        bq_date_start = st.date_input(
            "Start Date",
            value=date.today() - timedelta(days=90),
            help="Filter data from this date (pushes filter to BigQuery)"
        )
    with col2:
        bq_date_end = st.date_input(
            "End Date",
            value=date.today(),
            help="Filter data until this date"
        )

    # Pre-filter options
    bq_countries = st.sidebar.multiselect(
        "Pre-filter Countries",
        options=["CL", "AR", "BR", "MX", "CO", "PE"],
        default=[],
        help="Filter countries in BigQuery (leave empty for all)"
    )

    bq_channels = st.sidebar.multiselect(
        "Pre-filter Channels",
        options=["App", "Web"],
        default=[],
        help="Filter channels in BigQuery (leave empty for all)"
    )

    # Sampling option for quick exploration
    bq_sample = st.sidebar.slider(
        "Sample % (for testing)",
        min_value=0,
        max_value=100,
        value=0,
        step=5,
        help="Sample a percentage of data for quick exploration. 0 = use all filtered data"
    )

    bq_limit = st.sidebar.number_input(
        "Row Limit (for testing)",
        min_value=0,
        max_value=1000000,
        value=0,
        step=10000,
        help="Limit rows after filtering. 0 = no limit (use with caution on large datasets!)"
    )

    # Show estimated query info
    if bq_date_start and bq_date_end:
        days_diff = (bq_date_end - bq_date_start).days
        st.sidebar.info(f"üìÖ Date range: {days_diff} days")

    # Estimate data volume
    if bq_sample > 0:
        st.sidebar.info(f"üìä Using {bq_sample}% sample of data")
    elif bq_limit > 0:
        st.sidebar.info(f"üìä Limited to {bq_limit:,} rows")
    else:
        st.sidebar.warning("‚ö†Ô∏è No sampling or limit - may load large dataset!")

    st.sidebar.markdown("---")

    # Connect Button (for on-demand mode)
    load_button = st.sidebar.button("üöÄ Connect to BigQuery", type="primary", use_container_width=True)

    bq_params = {
        'project_id': bq_project,
        'table': bq_table,
        'credentials_path': bq_creds if bq_creds else None,
        'date_start': str(bq_date_start) if bq_date_start else None,
        'date_end': str(bq_date_end) if bq_date_end else None,
        'countries': bq_countries if bq_countries else None,
        'channels': bq_channels if bq_channels else None,
        'sample_percent': bq_sample if bq_sample > 0 else None,
        'limit': bq_limit if bq_limit > 0 else None
    }

# Initialize session state for data loading
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'loaded_df' not in st.session_state:
    st.session_state.loaded_df = None

# Load data
if data_source == "BigQuery":
    # Initialize connection when button is clicked
    if load_button or not st.session_state.get('bq_initialized', False):
        with st.spinner('üîÑ Connecting to BigQuery...'):
            initialize_bigquery_connection(bq_params)
            st.success(f"‚úÖ Connected to BigQuery: {bq_params['table']}")

    # Check if connection is initialized
    if not st.session_state.get('bq_initialized', False):
        # Show instructions if not connected yet
        st.info("üëÜ Configure your BigQuery settings in the sidebar and click **'Connect to BigQuery'**.")
        st.markdown("""
        ### On-Demand Query Mode üöÄ

        **This dashboard uses on-demand BigQuery queries** - data is fetched as needed for each section,
        rather than loading everything upfront. This means:

        ‚úÖ **Faster initial load** - No waiting for huge datasets
        ‚úÖ **Lower memory usage** - Only load what you're viewing
        ‚úÖ **Reduced costs** - Only query what you need
        ‚úÖ **Real-time filtering** - Filters apply directly in BigQuery

        **Quick Start:**
        1. Set your **Date Range** (filters data in BigQuery)
        2. Optionally add **Country** or **Channel** filters
        3. Click **'Connect to BigQuery'**
        4. Dashboard sections will query data on-demand!

        **Note:** Remove any sampling/limit settings for production use.
        """)
        st.stop()  # Stop execution until connected

    # BigQuery mode: store connection info, query on-demand per section
    # No df variable - each section will query directly

else:
    # CSV mode - load full data as before
    with st.spinner('Loading data...'):
        df = load_and_process_data(data_source, bq_params)

# Sidebar filters
st.sidebar.header("Filters")

# Determine if we're in BigQuery mode
is_bigquery_mode = data_source == "BigQuery" and st.session_state.get('bq_initialized', False)

# Date range filter
if is_bigquery_mode:
    min_date = st.session_state.bq_min_date
    max_date = st.session_state.bq_max_date
else:
    min_date = df['date'].min()
    max_date = df['date'].max()

date_range = st.sidebar.date_input(
    "Date Range",
    value=(min_date, max_date),
    min_value=min_date,
    max_value=max_date,
    key='date_range'
)

# Convert to datetime
if len(date_range) == 2:
    start_date, end_date = date_range
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
else:
    start_date = pd.to_datetime(date_range[0])
    end_date = pd.to_datetime(date_range[0])

# Period comparison (disabled in BigQuery mode for now)
if not is_bigquery_mode:
    st.sidebar.subheader("Period Comparison")
    enable_comparison = st.sidebar.checkbox("Enable Period Comparison", value=False)

    comparison_date_range = None
    if enable_comparison:
        comparison_date_range = st.sidebar.date_input(
            "Comparison Period",
            value=(min_date, min_date + timedelta(days=7)),
            min_value=min_date,
            max_value=max_date,
            key='comparison_date_range'
        )
        if len(comparison_date_range) == 2:
            comp_start_date, comp_end_date = comparison_date_range
            comp_start_date = pd.to_datetime(comp_start_date)
            comp_end_date = pd.to_datetime(comp_end_date)
        else:
            comp_start_date = pd.to_datetime(comparison_date_range[0])
            comp_end_date = pd.to_datetime(comparison_date_range[0])
else:
    enable_comparison = False
    comparison_date_range = None

# Country filter
st.sidebar.subheader("Geographic")
if is_bigquery_mode:
    # Use predefined list or query from BigQuery
    country_options = ['CL', 'AR', 'BR', 'MX', 'CO', 'PE']
else:
    country_options = sorted(df['country'].unique())

countries = st.sidebar.multiselect(
    "Countries",
    options=country_options,
    default=None
)

# Channel filter
st.sidebar.subheader("Channel")
if is_bigquery_mode:
    # Use predefined list
    channel_options = ['App', 'Web']
else:
    channel_options = sorted(df['channel'].unique())

channels = st.sidebar.multiselect(
    "Channels",
    options=channel_options,
    default=None
)

# Number of words filter
st.sidebar.subheader("Query Length")
word_options = ['1', '2', '3', '4+']
selected_words = st.sidebar.multiselect(
    "Number of Words",
    options=word_options,
    default=None
)

# Attribute combination filter (disabled in BigQuery mode - too expensive to query all combos)
if not is_bigquery_mode:
    st.sidebar.subheader("Search Patterns")
    df_with_combos = dp.add_attribute_combination_column(df)
    top_combos = df_with_combos['attribute_combination'].value_counts().head(20).index.tolist()
    selected_combos = st.sidebar.multiselect(
        "Attribute Combinations",
        options=top_combos,
        default=None,
        help="Filter by specific attribute combinations used in searches"
    )
else:
    selected_combos = None

# Attribute filters
st.sidebar.subheader("Attributes")

# Individual attribute toggles (removed - only using attribute combinations)
attr_cols = dp.get_attribute_columns()
selected_attributes = {}  # Empty dict - no individual attribute filters

# Number of attributes filter
with st.sidebar.expander("Number of Attributes", expanded=False):
    if is_bigquery_mode:
        max_attrs = 8  # Known max from schema
    else:
        max_attrs = int(df['n_attributes'].max())

    n_attrs_range = st.slider(
        "Attribute Count Range",
        min_value=0,
        max_value=max_attrs,
        value=(0, max_attrs),
        key='n_attrs_range'
    )

# Build base filters dictionary (used for BigQuery queries)
base_filters = {
    'date_start': str(start_date.date()),
    'date_end': str(end_date.date()),
    'countries': countries if countries else None,
    'channels': channels if channels else None,
    'n_attributes_range': n_attrs_range,
    'word_counts': selected_words if selected_words else None,
    'attribute_combinations': selected_combos if selected_combos else None
}

# Apply filters (CSV mode only - BigQuery queries on-demand)
if not is_bigquery_mode:
    filtered_df = dp.filter_data(
        df,
        date_range=(start_date, end_date),
        countries=countries if countries else None,
        channels=channels if channels else None,
        attributes={k: v for k, v in selected_attributes.items() if v},
        n_attributes_range=n_attrs_range,
        word_counts=selected_words if selected_words else None,
        attribute_combinations=selected_combos if selected_combos else None
    )

    # Get comparison dataset if enabled
    comparison_df = None
    if enable_comparison and comparison_date_range:
        comparison_df = dp.filter_data(
            df,
            date_range=(comp_start_date, comp_end_date),
            countries=countries if countries else None,
            channels=channels if channels else None,
            attributes={k: v for k, v in selected_attributes.items() if v},
            n_attributes_range=n_attrs_range,
            word_counts=selected_words if selected_words else None,
            attribute_combinations=selected_combos if selected_combos else None
        )
else:
    # BigQuery mode: no pre-filtered dataframe
    filtered_df = None
    comparison_df = None

# Main content area
# Skip this check in BigQuery mode (we query on-demand)
if not is_bigquery_mode and len(filtered_df) == 0:
    st.warning("No data available for the selected filters. Please adjust your filters.")
else:
    # Section 1: Overview KPIs
    st.header("Overview KPIs")

    # Get KPIs based on mode
    if is_bigquery_mode:
        # Query BigQuery for KPIs
        query = dp.build_kpi_query(st.session_state.bq_params['table'], base_filters)
        result_df = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate KPIs from BigQuery results
        total_queries = int(result_df['total_queries'].iloc[0]) if result_df['total_queries'].iloc[0] else 0
        total_revenue = float(result_df['total_revenue'].iloc[0]) if result_df['total_revenue'].iloc[0] else 0.0
        total_purchases = int(result_df['total_purchases'].iloc[0]) if result_df['total_purchases'].iloc[0] else 0
        total_queries_pdp = int(result_df['total_queries_pdp'].iloc[0]) if result_df['total_queries_pdp'].iloc[0] else 0

        kpis = {
            'total_queries': total_queries,
            'total_revenue': total_revenue,
            'avg_ctr': float(total_queries_pdp / total_queries) if total_queries > 0 else 0.0,
            'avg_conversion': float(total_purchases / total_queries) if total_queries > 0 else 0.0,
            'revenue_per_query': float(total_revenue / total_queries) if total_queries > 0 else 0.0,
            'total_purchases': total_purchases,
            'avg_order_value': float(total_revenue / total_purchases) if total_purchases > 0 else 0.0
        }
    else:
        # CSV mode - use existing logic
        kpis = dp.get_kpis(filtered_df)

    # Note: Comparison mode not supported in BigQuery mode yet
    if is_bigquery_mode and enable_comparison:
        st.info("üí° Comparison mode is not yet available in BigQuery mode. Using current period only.")
        enable_comparison = False

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        delta_queries = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = kpis['total_queries'] - comp_kpis['total_queries']
            pct = (change / comp_kpis['total_queries'] * 100) if comp_kpis['total_queries'] > 0 else 0
            delta_queries = f"{change:+,.0f} ({pct:+.1f}%)"

        st.metric(
            "Total Queries",
            f"{kpis['total_queries']:,}",
            delta=delta_queries
        )

    with col2:
        delta_revenue = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = kpis['total_revenue'] - comp_kpis['total_revenue']
            pct = (change / comp_kpis['total_revenue'] * 100) if comp_kpis['total_revenue'] > 0 else 0
            delta_revenue = f"${change:+,.2f} ({pct:+.1f}%)"

        st.metric(
            "Total Revenue",
            f"${kpis['total_revenue']:,.2f}",
            delta=delta_revenue
        )

    with col3:
        delta_conv = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = (kpis['avg_conversion'] - comp_kpis['avg_conversion']) * 100
            delta_conv = f"{change:+.2f}%"

        st.metric(
            "Conversion Rate",
            f"{kpis['avg_conversion']*100:.2f}%",
            delta=delta_conv
        )

    with col4:
        delta_rpq = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = kpis['revenue_per_query'] - comp_kpis['revenue_per_query']
            delta_rpq = f"${change:+.2f}"

        st.metric(
            "Revenue per Query",
            f"${kpis['revenue_per_query']:.2f}",
            delta=delta_rpq
        )

    st.divider()

    # Additional KPIs row
    col5, col6, col7, col8 = st.columns(4)

    with col5:
        delta_ctr = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = (kpis['avg_ctr'] - comp_kpis['avg_ctr']) * 100
            delta_ctr = f"{change:+.2f}%"

        st.metric(
            "Avg Click-Through Rate",
            f"{kpis['avg_ctr']*100:.2f}%",
            delta=delta_ctr
        )

    with col6:
        delta_purchases = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = kpis['total_purchases'] - comp_kpis['total_purchases']
            pct = (change / comp_kpis['total_purchases'] * 100) if comp_kpis['total_purchases'] > 0 else 0
            delta_purchases = f"{change:+,.0f} ({pct:+.1f}%)"

        st.metric(
            "Total Purchases",
            f"{kpis['total_purchases']:,}",
            delta=delta_purchases
        )

    with col7:
        delta_aov = None
        if enable_comparison and comparison_df is not None:
            comp_kpis = dp.get_kpis(comparison_df)
            change = kpis['avg_order_value'] - comp_kpis['avg_order_value']
            delta_aov = f"${change:+.2f}"

        st.metric(
            "Avg Order Value",
            f"${kpis['avg_order_value']:.2f}",
            delta=delta_aov
        )

    st.divider()

    # Section 2: Trend Analysis
    st.header("Trend Analysis")

    # Frequency selector
    freq_options = {
        'Daily': 'D',
        'Weekly': 'W',
        'Monthly': 'M'
    }
    freq_label = st.selectbox("Time Granularity", options=list(freq_options.keys()), index=0)
    freq = freq_options[freq_label]

    # Aggregate time series data based on mode
    if is_bigquery_mode:
        # Query BigQuery for time series
        query = dp.build_timeseries_query(st.session_state.bq_params['table'], base_filters, freq=freq)
        ts_data = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate derived metrics
        ts_data['ctr'] = np.where(ts_data['queries'] > 0, ts_data['queries_pdp'] / ts_data['queries'], 0)
        ts_data['a2c_rate'] = np.where(ts_data['queries'] > 0, ts_data['queries_a2c'] / ts_data['queries'], 0)
        ts_data['conversion_rate'] = np.where(ts_data['queries'] > 0, ts_data['purchases'] / ts_data['queries'], 0)
        ts_data['revenue_per_query'] = np.where(ts_data['queries'] > 0, ts_data['gross_purchase'] / ts_data['queries'], 0)
        ts_data['avg_order_value'] = np.where(ts_data['purchases'] > 0, ts_data['gross_purchase'] / ts_data['purchases'], 0)

        ts_comparison = None  # Comparison not supported in BigQuery mode
    else:
        # CSV mode - use existing logic
        ts_data = dp.aggregate_time_series(filtered_df, freq=freq)
        ts_comparison = dp.aggregate_time_series(comparison_df, freq=freq) if enable_comparison and comparison_df is not None else None

    # Metric selector for trends
    trend_metrics = st.multiselect(
        "Select Metrics to Display",
        options=['queries', 'revenue_per_query', 'ctr', 'conversion_rate', 'a2c_rate'],
        default=['queries', 'revenue_per_query']
    )

    if trend_metrics:
        for metric in trend_metrics:
            metric_names = {
                'queries': 'Total Queries',
                'revenue_per_query': 'Revenue per Query',
                'ctr': 'Click-Through Rate',
                'conversion_rate': 'Conversion Rate',
                'a2c_rate': 'Add-to-Cart Rate'
            }

            fig = viz.create_trend_chart(
                ts_data,
                metric,
                title=f"{metric_names.get(metric, metric)} Over Time",
                yaxis_title=metric_names.get(metric, metric),
                comparison_df=ts_comparison
            )
            st.plotly_chart(fig, use_container_width=True)

    st.divider()

    # Section 3: Channel Performance
    st.header("Channel Performance")

    # Get channel data based on mode
    if is_bigquery_mode:
        # Query BigQuery for channel performance
        query = dp.build_channel_query(st.session_state.bq_params['table'], base_filters)
        channel_data = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate derived metrics
        channel_data['ctr'] = np.where(channel_data['queries'] > 0, channel_data['queries_pdp'] / channel_data['queries'], 0)
        channel_data['a2c_rate'] = np.where(channel_data['queries'] > 0, channel_data['queries_a2c'] / channel_data['queries'], 0)
        channel_data['conversion_rate'] = np.where(channel_data['queries'] > 0, channel_data['purchases'] / channel_data['queries'], 0)
        channel_data['revenue_per_query'] = np.where(channel_data['queries'] > 0, channel_data['gross_purchase'] / channel_data['queries'], 0)
        channel_data['avg_order_value'] = np.where(channel_data['purchases'] > 0, channel_data['gross_purchase'] / channel_data['purchases'], 0)

        has_channel_data = len(channel_data) > 0
    else:
        has_channel_data = 'channel' in filtered_df.columns and len(filtered_df['channel'].unique()) > 0

    if has_channel_data:
        if is_bigquery_mode:
            # Use BigQuery data for chart
            fig_channel = viz.create_channel_comparison(channel_data)
        else:
            # Use CSV data for chart
            fig_channel = viz.create_channel_comparison(filtered_df)

        st.plotly_chart(fig_channel, use_container_width=True)

        # Detailed channel table
        st.subheader("Channel Details")

        if is_bigquery_mode:
            channel_details = channel_data.copy()
        else:
            channel_details = dp.aggregate_by_dimension(filtered_df, 'channel')

        # Sort by queries descending
        channel_details = channel_details.sort_values('queries', ascending=False).reset_index(drop=True)

        # Calculate % of total queries and cumulative %
        total_queries = channel_details['queries'].sum()
        channel_details['query_pct'] = (channel_details['queries'] / total_queries * 100)
        channel_details['cumulative_queries'] = channel_details['queries'].cumsum()
        channel_details['cumulative_pct'] = (channel_details['cumulative_queries'] / total_queries * 100)

        # Select and reorder columns
        channel_details = channel_details[[
            'channel', 'queries', 'query_pct', 'cumulative_pct', 'ctr', 'a2c_rate', 'conversion_rate',
            'revenue_per_query', 'purchases', 'gross_purchase'
        ]]

        # Format columns
        channel_details['query_pct'] = channel_details['query_pct'].apply(lambda x: f"{x:.1f}%")
        channel_details['cumulative_pct'] = channel_details['cumulative_pct'].apply(lambda x: f"{x:.1f}%")
        channel_details['ctr'] = channel_details['ctr'].apply(lambda x: f"{x*100:.2f}%")
        channel_details['a2c_rate'] = channel_details['a2c_rate'].apply(lambda x: f"{x*100:.2f}%")
        channel_details['conversion_rate'] = channel_details['conversion_rate'].apply(lambda x: f"{x*100:.2f}%")
        channel_details['revenue_per_query'] = channel_details['revenue_per_query'].apply(lambda x: f"${x:.2f}")
        channel_details['gross_purchase'] = channel_details['gross_purchase'].apply(lambda x: f"${x:,.2f}")

        # Rename columns for display
        channel_details = channel_details.rename(columns={
            'query_pct': '% of queries',
            'cumulative_pct': 'cumulative %'
        })

        st.dataframe(channel_details, use_container_width=True, hide_index=True)
    else:
        st.info("No channel data available with current filters.")

    st.divider()

    # Section 4: Search Query Length Analysis
    st.header("Search Query Length Analysis")

    # Get word count data based on mode
    if is_bigquery_mode:
        # Query BigQuery for word count analysis
        query = dp.build_word_count_query(st.session_state.bq_params['table'], base_filters)
        n_words_grouped = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate derived metrics
        n_words_grouped['ctr'] = np.where(n_words_grouped['queries'] > 0, n_words_grouped['queries_pdp'] / n_words_grouped['queries'], 0)
        n_words_grouped['a2c_rate'] = np.where(n_words_grouped['queries'] > 0, n_words_grouped['queries_a2c'] / n_words_grouped['queries'], 0)
        n_words_grouped['conversion_rate'] = np.where(n_words_grouped['queries'] > 0, n_words_grouped['purchases'] / n_words_grouped['queries'], 0)
        n_words_grouped['revenue_per_query'] = np.where(n_words_grouped['queries'] > 0, n_words_grouped['gross_purchase'] / n_words_grouped['queries'], 0)
        n_words_grouped['avg_order_value'] = np.where(n_words_grouped['purchases'] > 0, n_words_grouped['gross_purchase'] / n_words_grouped['purchases'], 0)

        has_word_count_data = len(n_words_grouped) > 0
    else:
        has_word_count_data = 'n_words_normalized' in filtered_df.columns

    if has_word_count_data:
        st.subheader("Performance by Number of Words (1, 2, 3, 4+)")

        # Get aggregated data (only for CSV mode)
        if not is_bigquery_mode:
            n_words_grouped = dp.aggregate_by_word_count_grouped(filtered_df)

        # Create visualization
        fig_n_words = viz.create_n_words_chart(n_words_grouped, word_col='n_words_grouped')
        st.plotly_chart(fig_n_words, use_container_width=True)

        # Detailed table by grouped n_words
        st.subheader("Details by Number of Words")

        # Calculate % of total queries and cumulative %
        total_queries = n_words_grouped['queries'].sum()
        n_words_grouped['query_pct'] = (n_words_grouped['queries'] / total_queries * 100)
        n_words_grouped['cumulative_queries'] = n_words_grouped['queries'].cumsum()
        n_words_grouped['cumulative_pct'] = (n_words_grouped['cumulative_queries'] / total_queries * 100)

        n_words_details = n_words_grouped[[
            'n_words_grouped', 'queries', 'query_pct', 'cumulative_pct', 'ctr', 'a2c_rate', 'conversion_rate',
            'revenue_per_query', 'purchases', 'gross_purchase'
        ]].copy()

        # Format columns
        n_words_details['query_pct'] = n_words_details['query_pct'].apply(lambda x: f"{x:.1f}%")
        n_words_details['cumulative_pct'] = n_words_details['cumulative_pct'].apply(lambda x: f"{x:.1f}%")
        n_words_details['ctr'] = n_words_details['ctr'].apply(lambda x: f"{x*100:.2f}%")
        n_words_details['a2c_rate'] = n_words_details['a2c_rate'].apply(lambda x: f"{x*100:.2f}%")
        n_words_details['conversion_rate'] = n_words_details['conversion_rate'].apply(lambda x: f"{x*100:.2f}%")
        n_words_details['revenue_per_query'] = n_words_details['revenue_per_query'].apply(lambda x: f"${x:.2f}")
        n_words_details['gross_purchase'] = n_words_details['gross_purchase'].apply(lambda x: f"${x:,.2f}")

        # Rename columns for clarity
        n_words_details = n_words_details.rename(columns={
            'n_words_grouped': 'num_words',
            'query_pct': '% of queries',
            'cumulative_pct': 'cumulative %'
        })

        st.dataframe(n_words_details, use_container_width=True, hide_index=True)

        # Add info box about grouping
        st.info("üìä Queries with 4 or more words are grouped together as '4+' for clearer visualization due to lower individual volumes.")
    else:
        st.info("No word count data available with current filters.")

    st.divider()

    # Section 5: Attribute Analysis
    st.header("Attribute Analysis")

    st.subheader("Performance by Attribute Combinations")

    # Control for number of combinations to show
    top_n = st.slider("Number of top combinations to display", min_value=10, max_value=30, value=15, step=5)

    # Get attribute combination data based on mode
    if is_bigquery_mode:
        # Query BigQuery for attribute combinations
        query = dp.build_attribute_combination_query(st.session_state.bq_params['table'], base_filters, top_n=top_n)
        attr_combos = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate derived metrics
        attr_combos['ctr'] = np.where(attr_combos['queries'] > 0, attr_combos['queries_pdp'] / attr_combos['queries'], 0)
        attr_combos['a2c_rate'] = np.where(attr_combos['queries'] > 0, attr_combos['queries_a2c'] / attr_combos['queries'], 0)
        attr_combos['conversion_rate'] = np.where(attr_combos['queries'] > 0, attr_combos['purchases'] / attr_combos['queries'], 0)
        attr_combos['revenue_per_query'] = np.where(attr_combos['queries'] > 0, attr_combos['gross_purchase'] / attr_combos['queries'], 0)
        attr_combos['avg_order_value'] = np.where(attr_combos['purchases'] > 0, attr_combos['gross_purchase'] / attr_combos['purchases'], 0)
    else:
        # CSV mode - use existing logic
        attr_combos = dp.aggregate_by_attribute_combination(filtered_df, top_n=top_n)

    if not attr_combos.empty:
        fig_attr_combos = viz.create_attribute_combination_chart(attr_combos)
        st.plotly_chart(fig_attr_combos, use_container_width=True)

        # Detailed table
        st.subheader(f"Top {top_n} Attribute Combination Details")

        # Calculate % of total queries and cumulative %
        # Use the KPI total we already calculated for this filtered period
        total_queries_all = kpis['total_queries']
        attr_combos['query_pct'] = (attr_combos['queries'] / total_queries_all * 100)
        attr_combos['cumulative_queries'] = attr_combos['queries'].cumsum()
        attr_combos['cumulative_pct'] = (attr_combos['cumulative_queries'] / total_queries_all * 100)

        attr_combo_display = attr_combos[[
            'attribute_combination', 'queries', 'query_pct', 'cumulative_pct', 'ctr', 'a2c_rate', 'conversion_rate',
            'revenue_per_query', 'purchases', 'gross_purchase'
        ]].copy()

        # Format columns
        attr_combo_display['query_pct'] = attr_combo_display['query_pct'].apply(lambda x: f"{x:.1f}%")
        attr_combo_display['cumulative_pct'] = attr_combo_display['cumulative_pct'].apply(lambda x: f"{x:.1f}%")
        attr_combo_display['ctr'] = attr_combo_display['ctr'].apply(lambda x: f"{x*100:.2f}%")
        attr_combo_display['a2c_rate'] = attr_combo_display['a2c_rate'].apply(lambda x: f"{x*100:.2f}%")
        attr_combo_display['conversion_rate'] = attr_combo_display['conversion_rate'].apply(lambda x: f"{x*100:.2f}%")
        attr_combo_display['revenue_per_query'] = attr_combo_display['revenue_per_query'].apply(lambda x: f"${x:.2f}")
        attr_combo_display['gross_purchase'] = attr_combo_display['gross_purchase'].apply(lambda x: f"${x:,.2f}")

        # Rename columns for clarity
        attr_combo_display = attr_combo_display.rename(columns={
            'attribute_combination': 'combination',
            'query_pct': '% of queries',
            'cumulative_pct': 'cumulative %'
        })

        st.dataframe(attr_combo_display, use_container_width=True, hide_index=True)

        # Add insight box
        st.info(f"üí° Showing top {top_n} attribute combinations by query volume. " +
               "Combinations like 'Marca + Color' show how customers combine different attributes in their searches.")
    else:
        st.info("No attribute combination data available with current filters.")

    st.divider()

    # Section 6: Search Term Explorer
    st.header("Search Term Explorer")

    # Top search terms chart
    st.subheader("Top Search Terms")

    col_metric, col_top_n = st.columns([3, 1])
    with col_metric:
        top_metric = st.selectbox(
            "Rank by",
            options=['queries', 'gross_purchase', 'purchases', 'queries_pdp'],
            index=0,
            key='top_metric'
        )
    with col_top_n:
        top_n_chart = st.number_input("Show top", min_value=5, max_value=100, value=20, step=5)

    # Get search term data based on mode
    if is_bigquery_mode:
        # Query BigQuery for search terms (get more than needed for local filtering/sorting)
        query = dp.build_search_terms_query(
            st.session_state.bq_params['table'],
            base_filters,
            search_filter=None,  # Don't filter in query - do it in pandas for flexibility
            sort_by='queries',
            ascending=False,
            limit=1000  # Get top 1000 for local manipulation
        )
        search_term_agg = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

        # Calculate derived metrics
        search_term_agg['ctr'] = np.where(search_term_agg['queries'] > 0, search_term_agg['queries_pdp'] / search_term_agg['queries'], 0)
        search_term_agg['a2c_rate'] = np.where(search_term_agg['queries'] > 0, search_term_agg['queries_a2c'] / search_term_agg['queries'], 0)
        search_term_agg['conversion_rate'] = np.where(search_term_agg['queries'] > 0, search_term_agg['purchases'] / search_term_agg['queries'], 0)
        search_term_agg['revenue_per_query'] = np.where(search_term_agg['queries'] > 0, search_term_agg['gross_purchase'] / search_term_agg['queries'], 0)
        search_term_agg['avg_order_value'] = np.where(search_term_agg['purchases'] > 0, search_term_agg['gross_purchase'] / search_term_agg['purchases'], 0)
    else:
        # CSV mode - use existing logic
        search_term_agg = dp.aggregate_by_search_term(filtered_df)

    if not search_term_agg.empty:
        fig_top_searches = viz.create_top_searches_chart(search_term_agg, metric=top_metric, top_n=top_n_chart)
        st.plotly_chart(fig_top_searches, use_container_width=True)

        # Detailed search term table with multi-level drill-down
        st.subheader("Search Term Details")

        # Search box
        search_filter = st.text_input("Filter search terms (contains)", value="")

        # Apply search filter
        if search_filter:
            display_df = search_term_agg[search_term_agg['search_term'].str.contains(search_filter, case=False, na=False)]
        else:
            display_df = search_term_agg

        # Sort options
        sort_col, sort_order = st.columns([3, 1])
        with sort_col:
            sort_by = st.selectbox(
                "Sort by",
                options=['queries', 'ctr', 'a2c_rate', 'conversion_rate', 'revenue_per_query', 'gross_purchase', 'purchases'],
                index=0,
                key='sort_by'
            )
        with sort_order:
            ascending = st.checkbox("Ascending", value=False, key='ascending')

        display_df = display_df.sort_values(sort_by, ascending=ascending).reset_index(drop=True)

        # Calculate cumulative % based on current sort (by queries)
        total_queries_st = display_df['queries'].sum()
        display_df['query_pct'] = (display_df['queries'] / total_queries_st * 100)
        display_df['cumulative_queries'] = display_df['queries'].cumsum()
        display_df['cumulative_pct'] = (display_df['cumulative_queries'] / total_queries_st * 100)

        # Format for display
        display_formatted = display_df.copy()
        display_formatted['query_pct'] = display_formatted['query_pct'].apply(lambda x: f"{x:.2f}%")
        display_formatted['cumulative_pct'] = display_formatted['cumulative_pct'].apply(lambda x: f"{x:.1f}%")
        display_formatted['ctr'] = display_formatted['ctr'].apply(lambda x: f"{x*100:.2f}%")
        display_formatted['a2c_rate'] = display_formatted['a2c_rate'].apply(lambda x: f"{x*100:.2f}%")
        display_formatted['conversion_rate'] = display_formatted['conversion_rate'].apply(lambda x: f"{x*100:.2f}%")
        display_formatted['revenue_per_query'] = display_formatted['revenue_per_query'].apply(lambda x: f"${x:.2f}")
        display_formatted['gross_purchase'] = display_formatted['gross_purchase'].apply(lambda x: f"${x:,.2f}")
        display_formatted['avg_order_value'] = display_formatted['avg_order_value'].apply(lambda x: f"${x:.2f}")

        # Rename columns for display
        display_formatted = display_formatted.rename(columns={
            'query_pct': '% of queries',
            'cumulative_pct': 'cumulative %'
        })

        # Select columns to display
        display_cols = [
            'search_term', 'queries', '% of queries', 'cumulative %', 'ctr', 'a2c_rate', 'conversion_rate',
            'revenue_per_query', 'purchases', 'gross_purchase', 'avg_order_value'
        ]

        st.dataframe(
            display_formatted[display_cols],
            use_container_width=True,
            hide_index=True,
            height=600
        )

        # Download button
        csv = display_df.to_csv(index=False)
        st.download_button(
            label="Download Search Term Data (CSV)",
            data=csv,
            file_name=f"search_terms_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )
    else:
        st.info("No search term data available with current filters.")

    st.divider()

    # Section 7: Hierarchical Pivot Table
    st.header("Hierarchical Pivot Table")
    st.write("Drill down through multiple dimensions with expandable rows (like Excel pivot tables)")

    # Initialize session state for expanded rows
    if 'expanded_paths' not in st.session_state:
        st.session_state.expanded_paths = set()

    # Get available dimensions
    dimensions = dp.get_available_dimensions()

    # Initialize session state for dimension order
    if 'dimension_order' not in st.session_state:
        st.session_state.dimension_order = ['Attribute Combination', 'Number of Words']

    # Dimension selector
    st.subheader("Select Dimensions (in hierarchy order)")

    # Available dimensions selector
    available_dims = [d for d in dimensions.keys() if d not in st.session_state.dimension_order]
    add_dimension = st.selectbox(
        "Add dimension",
        options=['-- Select to add --'] + available_dims,
        key='add_dimension_select'
    )

    if add_dimension != '-- Select to add --' and len(st.session_state.dimension_order) < 4:
        st.session_state.dimension_order.append(add_dimension)
        st.rerun()

    # Display current dimensions with reorder controls
    if len(st.session_state.dimension_order) > 0:
        st.write("**Current hierarchy order:**")

        for idx, dim_name in enumerate(st.session_state.dimension_order):
            cols = st.columns([3, 0.5, 0.5, 0.5])

            with cols[0]:
                st.write(f"{idx + 1}. {dim_name}")

            with cols[1]:
                if idx > 0:
                    if st.button("‚¨Ü", key=f"up_{idx}"):
                        st.session_state.dimension_order[idx], st.session_state.dimension_order[idx-1] = \
                            st.session_state.dimension_order[idx-1], st.session_state.dimension_order[idx]
                        st.rerun()

            with cols[2]:
                if idx < len(st.session_state.dimension_order) - 1:
                    if st.button("‚¨á", key=f"down_{idx}"):
                        st.session_state.dimension_order[idx], st.session_state.dimension_order[idx+1] = \
                            st.session_state.dimension_order[idx+1], st.session_state.dimension_order[idx]
                        st.rerun()

            with cols[3]:
                if st.button("‚úï", key=f"remove_{idx}"):
                    st.session_state.dimension_order.pop(idx)
                    st.rerun()

    selected_dimension_names = st.session_state.dimension_order

    # Sorting controls
    # Metric selection
    available_metrics = ['Queries', 'CTR', 'A2C', 'Conv', '$/Query', 'Revenue', 'AOV']
    selected_metrics = st.multiselect(
        "Select metrics to display",
        options=available_metrics,
        default=['Queries', 'CTR', 'Conv', '$/Query', 'Revenue'],
        key='selected_metrics_hierarchy'
    )

    # Sorting controls - one per dimension level
    st.subheader("Sorting by Dimension Level")

    # Initialize session state for sort options if not exists
    if 'sort_options_per_level' not in st.session_state:
        st.session_state.sort_options_per_level = {}

    # Show sort controls for each selected dimension
    sort_options_per_level = {}
    for idx, dim_name in enumerate(selected_dimension_names):
        col_dim, col_metric, col_order = st.columns([2, 2, 1])
        with col_dim:
            st.markdown(f"**Level {idx + 1}: {dim_name}**")
        with col_metric:
            sort_metric = st.selectbox(
                f"Sort by",
                options=['Queries', 'Conversion Rate', 'CTR', 'A2C Rate', 'Revenue per Query', 'Revenue', 'AOV'],
                index=0,
                key=f'sort_metric_level_{idx}',
                label_visibility='collapsed'
            )
        with col_order:
            sort_asc = st.checkbox("Asc", value=False, key=f'sort_asc_level_{idx}')

        sort_options_per_level[idx] = {
            'metric': sort_metric,
            'ascending': sort_asc
        }

    # Global settings
    col_thresh, col_search = st.columns([2, 2])
    with col_thresh:
        cumulative_threshold = st.slider(
            "Cumulative threshold %",
            min_value=50,
            max_value=100,
            value=80,
            step=5,
            key='cumulative_threshold_hierarchy',
            help="Show rows until this cumulative % is reached, aggregate the rest as 'Other'"
        )

    # Search term aggregation threshold
    col_search1, col_search2 = st.columns([2, 3])
    with col_search1:
        min_queries_for_search_terms = st.number_input(
            "Min queries for search terms",
            min_value=1,
            max_value=100,
            value=5,
            step=1,
            key='min_queries_search_terms',
            help="Search terms with fewer queries will be aggregated into 'Other'"
        )

    # Map sort metric to column name
    sort_metric_map = {
        'Queries': 'queries',
        'Conversion Rate': 'conversion_rate',
        'CTR': 'ctr',
        'A2C Rate': 'a2c_rate',
        'Revenue per Query': 'revenue_per_query',
        'Revenue': 'gross_purchase',
        'Purchases': 'purchases',
        'AOV': 'avg_order_value'
    }

    # Convert sort options to column names for each level
    sort_configs = []
    for idx in range(len(selected_dimension_names)):
        if idx in sort_options_per_level:
            opts = sort_options_per_level[idx]
            sort_configs.append({
                'sort_by': sort_metric_map[opts['metric']],
                'ascending': opts['ascending']
            })
        else:
            # Default to sorting by queries descending
            sort_configs.append({
                'sort_by': 'queries',
                'ascending': False
            })

    if len(selected_dimension_names) == 0:
        # Show Grand Total when no dimensions selected in same table format
        st.write("**Grand Total** (no dimension breakdown selected)")

        # Calculate overall totals based on mode
        if is_bigquery_mode:
            # Use the KPIs we already calculated
            total_queries = kpis['total_queries']
            total_purchases = kpis['total_purchases']
            total_revenue = kpis['total_revenue']
            # Query for PDP and A2C if needed
            query = dp.build_kpi_query(st.session_state.bq_params['table'], base_filters)
            result_df = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)
            total_queries_pdp = int(result_df['total_queries_pdp'].iloc[0]) if result_df['total_queries_pdp'].iloc[0] else 0
            total_queries_a2c = int(result_df['total_queries_a2c'].iloc[0]) if result_df['total_queries_a2c'].iloc[0] else 0
        else:
            # CSV mode
            total_queries = filtered_df['queries'].sum()
            total_queries_pdp = filtered_df['queries_pdp'].sum()
            total_queries_a2c = filtered_df['queries_a2c'].sum()
            total_purchases = filtered_df['purchases'].sum()
            total_revenue = filtered_df['gross_purchase'].sum()

        # Calculate rates
        overall_ctr = total_queries_pdp / total_queries if total_queries > 0 else 0
        overall_a2c = total_queries_a2c / total_queries if total_queries > 0 else 0
        overall_conv = total_purchases / total_queries if total_queries > 0 else 0
        overall_rpq = total_revenue / total_queries if total_queries > 0 else 0
        overall_aov = total_revenue / total_purchases if total_purchases > 0 else 0

        # Build metric config (same as in the hierarchical table)
        metric_config = {
            'Queries': {'field': 'queries', 'value': total_queries, 'format': lambda x: f"{x:,.0f}"},
            'CTR': {'field': 'ctr', 'value': overall_ctr, 'format': lambda x: f"{x*100:.2f}%"},
            'A2C': {'field': 'a2c_rate', 'value': overall_a2c, 'format': lambda x: f"{x*100:.2f}%"},
            'Conv': {'field': 'conversion_rate', 'value': overall_conv, 'format': lambda x: f"{x*100:.2f}%"},
            '$/Query': {'field': 'revenue_per_query', 'value': overall_rpq, 'format': lambda x: f"${x:.2f}"},
            'Revenue': {'field': 'gross_purchase', 'value': total_revenue, 'format': lambda x: f"${x:,.0f}"},
            'AOV': {'field': 'avg_order_value', 'value': overall_aov, 'format': lambda x: f"${x:.2f}"}
        }

        # Display selected metrics
        metric_headers = [m for m in selected_metrics if m in metric_config]

        if len(metric_headers) > 0:
            # Fixed columns: button, dimension, % Total, Cum%(O), % Parent, Cum%(P)
            base_headers = ["", "Dimension", "% Total", "Cum%(O)", "% Parent", "Cum%(P)"]
            num_base_cols = len(base_headers)
            num_metric_cols = len(metric_headers)

            # Build full header list and column layout
            all_headers = base_headers + metric_headers
            col_widths = [0.3, 2, 0.7, 0.8, 0.8, 0.8] + [1] * num_metric_cols

            # Create header
            header_html = '<div style="display: flex; width: 100%; font-weight: bold; border-bottom: 2px solid rgba(128, 128, 128, 0.2); padding-bottom: 0.5rem; margin-bottom: 0.5rem;">'
            col_width_pcts = [w / sum(col_widths) * 100 for w in col_widths]
            for i, (header, width_pct) in enumerate(zip(all_headers, col_width_pcts)):
                padding = "0.25rem 0.5rem 0.25rem 0.75rem" if i > 0 else "0.25rem 0.5rem"
                header_html += f'<div style="flex: 0 0 {width_pct:.1f}%; padding: {padding};">{header}</div>'
            header_html += '</div>'
            st.markdown(header_html, unsafe_allow_html=True)

            # Create Grand Total row
            cols = st.columns(col_widths)

            # Empty button column
            with cols[0]:
                st.write("")

            # Dimension value
            with cols[1]:
                st.write("**GRAND TOTAL**")

            # % of Total (always 100%)
            with cols[2]:
                st.write("100.00%")

            # Cumulative % (overall) (always 100%)
            with cols[3]:
                st.write("100.00%")

            # % of Parent (N/A for grand total)
            with cols[4]:
                st.write("-")

            # Cumulative % (within parent) (N/A for grand total)
            with cols[5]:
                st.write("-")

            # Dynamic metrics
            for idx, metric_name in enumerate(metric_headers):
                config = metric_config[metric_name]
                with cols[num_base_cols + idx]:
                    st.write(config['format'](config['value']))

        else:
            st.info("Please select at least one metric to display")

        st.divider()
        st.caption("üí° **Tip**: Add dimensions above to see breakdown by different attributes")
    else:
        # Convert dimension names to keys
        selected_dimension_keys = [dimensions[name] for name in selected_dimension_names]

        # Build hierarchical data (both CSV and BigQuery modes)
        if is_bigquery_mode:
            # BigQuery mode - query for aggregated data by dimensions
            try:
                with st.spinner('Querying BigQuery for hierarchical data...'):
                    # Query BigQuery for aggregated data by dimensions
                    query = dp.build_hierarchical_query(
                        st.session_state.bq_params['table'],
                        base_filters,
                        selected_dimension_keys
                    )
                    bq_agg_data = dp.execute_cached_query(st.session_state.bq_client, query, ttl=300)

                    # Calculate total queries from KPIs (already cached)
                    total_queries = kpis['total_queries']

                    # Calculate derived metrics
                    bq_agg_data['ctr'] = np.where(bq_agg_data['queries'] > 0,
                                                   bq_agg_data['queries_pdp'] / bq_agg_data['queries'], 0)
                    bq_agg_data['a2c_rate'] = np.where(bq_agg_data['queries'] > 0,
                                                        bq_agg_data['queries_a2c'] / bq_agg_data['queries'], 0)
                    bq_agg_data['conversion_rate'] = np.where(bq_agg_data['queries'] > 0,
                                                               bq_agg_data['purchases'] / bq_agg_data['queries'], 0)
                    bq_agg_data['revenue_per_query'] = np.where(bq_agg_data['queries'] > 0,
                                                                 bq_agg_data['gross_purchase'] / bq_agg_data['queries'], 0)
                    bq_agg_data['avg_order_value'] = np.where(bq_agg_data['purchases'] > 0,
                                                               bq_agg_data['gross_purchase'] / bq_agg_data['purchases'], 0)

                    # Build hierarchical structure from aggregated data
                    hierarchical_data = dp.build_hierarchical_table(
                        bq_agg_data,
                        selected_dimension_keys,
                        sort_configs=sort_configs,
                        total_queries=total_queries,
                        cumulative_threshold=cumulative_threshold / 100.0
                    )
            except Exception as e:
                st.error(f"Error building BigQuery hierarchical table: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                hierarchical_data = pd.DataFrame()  # Empty DataFrame on error
        else:
            # CSV mode - use filtered_df
            try:
                with st.spinner('Building hierarchical table...'):
                    # Calculate total queries from filtered data
                    total_queries = int(filtered_df['queries'].sum())

                    hierarchical_data = dp.build_hierarchical_table(
                        filtered_df,
                        selected_dimension_keys,
                        sort_configs=sort_configs,
                        total_queries=total_queries,
                        cumulative_threshold=cumulative_threshold / 100.0
                    )
            except Exception as e:
                st.error(f"Error building CSV hierarchical table: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                hierarchical_data = pd.DataFrame()  # Empty DataFrame on error

        # Shared display logic for both modes
        if not hierarchical_data.empty:
            # Add expand/collapse controls
            col_exp1, col_exp2 = st.columns([1, 4])
            with col_exp1:
                if st.button("Expand All", key='expand_all'):
                            st.session_state.expanded_paths = set(hierarchical_data['path'].tolist())
                            st.rerun()
                    with col_exp2:
                        if st.button("Collapse All", key='collapse_all'):
                            st.session_state.expanded_paths = set()
                            st.rerun()
    
                    # Filter to visible rows only (based on expanded state)
                    visible_rows = []
                    for idx, row in hierarchical_data.iterrows():
                        # Level 0 is always visible
                        if row['level'] == 0:
                            visible_rows.append(row)
                        else:
                            # Check if parent is expanded
                            if row['parent_path'] in st.session_state.expanded_paths:
                                visible_rows.append(row)
    
                    # Limit number of visible rows for performance
                    max_visible = 100
                    if len(visible_rows) > max_visible:
                        st.warning(f"Showing first {max_visible} of {len(visible_rows)} visible rows for performance. Collapse some rows or adjust filters.")
                        visible_rows = visible_rows[:max_visible]
    
                    # Create display with expand/collapse buttons
                    if len(visible_rows) > 0:
                        # Calculate total queries for % calculation
                        total_queries_all = total_queries  # Already calculated above
    
                        st.write(f"**Showing {len(visible_rows)} rows** (click ‚ûï/‚ûñ to expand/collapse)")
    
                        # Build dynamic headers and column config based on selected metrics
                        metric_config = {
                            'Queries': {'field': 'queries', 'format': lambda x: f"{x:,}"},
                            'CTR': {'field': 'ctr', 'format': lambda x: f"{x*100:.2f}%"},
                            'A2C': {'field': 'a2c_rate', 'format': lambda x: f"{x*100:.2f}%"},
                            'Conv': {'field': 'conversion_rate', 'format': lambda x: f"{x*100:.2f}%"},
                            '$/Query': {'field': 'revenue_per_query', 'format': lambda x: f"${x:.2f}"},
                            'Revenue': {'field': 'gross_purchase', 'format': lambda x: f"${x:,.0f}"},
                            'AOV': {'field': 'avg_order_value', 'format': lambda x: f"${x:.2f}"}
                        }
    
                        # Fixed columns: button, dimension, % Total, Cum%(O), % Parent, Cum%(P)
                        base_headers = ["", "Dimension", "% Total", "Cum%(O)", "% Parent", "Cum%(P)"]
                        num_base_cols = len(base_headers)
    
                        # Dynamic metric columns
                        metric_headers = [m for m in selected_metrics if m in metric_config]
                        num_metric_cols = len(metric_headers)
    
                        # Build full header list and column layout
                        all_headers = base_headers + metric_headers
                        col_widths = [0.3, 2, 0.7, 0.8, 0.8, 0.8] + [1] * num_metric_cols
    
                        # Add CSS for sticky header
                        st.markdown("""
                            <style>
                            .hierarchy-table-header {
                                position: sticky;
                                top: 3.5rem;
                                background-color: var(--background-color);
                                z-index: 999;
                                padding: 0.5rem 0 0.5rem 0;
                                margin: -0.5rem 0 0.5rem 0;
                                border-bottom: 2px solid rgba(128, 128, 128, 0.2);
                            }
                            </style>
                        """, unsafe_allow_html=True)
    
                        # Create sticky header using container
                        st.markdown('<div class="hierarchy-table-header">', unsafe_allow_html=True)
    
                        # Build header as HTML flexbox to match Streamlit columns
                        # Note: Streamlit columns have internal spacing, so we don't add gap
                        header_html = '<div style="display: flex; width: 100%; font-weight: bold;">'
                        col_width_pcts = [w / sum(col_widths) * 100 for w in col_widths]
                        for i, (header, width_pct) in enumerate(zip(all_headers, col_width_pcts)):
                            # Add left padding to all but first column to match Streamlit spacing
                            padding = "0.25rem 0.5rem 0.25rem 0.75rem" if i > 0 else "0.25rem 0.5rem"
                            header_html += f'<div style="flex: 0 0 {width_pct:.1f}%; padding: {padding};">{header}</div>'
                        header_html += '</div>'
    
                        st.markdown(header_html, unsafe_allow_html=True)
                        st.markdown('</div>', unsafe_allow_html=True)
    
                        st.divider()
    
                        # Render each row with button and metrics
                        for row in visible_rows:
                            # Create columns dynamically
                            cols = st.columns(col_widths)
    
                            # Combined expand/drill button
                            with cols[0]:
                                if row['has_children']:
                                    is_expanded = row['path'] in st.session_state.expanded_paths
                                    button_key = f"toggle_{row['path']}"
    
                                    def toggle_expand(path=row['path'], expanded=is_expanded):
                                        if expanded:
                                            st.session_state.expanded_paths.discard(path)
                                        else:
                                            st.session_state.expanded_paths.add(path)
    
                                    st.button(
                                        "‚ûñ" if is_expanded else "‚ûï",
                                        key=button_key,
                                        on_click=toggle_expand
                                    )
                                else:
                                    st.write("")
    
                            # Dimension value with drill-down icon (clickable)
                            with cols[1]:
                                indent = "&nbsp;&nbsp;" * row['level']
    
                                # Check if this row is "Other"
                                is_other = row.get('is_other', False)
    
                                # Check if this row is currently showing drill-down
                                is_drilled = 'selected_drill_path' in st.session_state and st.session_state.selected_drill_path == row['path']
    
                                # Create a combined button with indentation
                                icon = "‚äï" if is_other else "üîç"
                                drill_icon = "üîΩ" if is_drilled else "‚ñ∂"
                                button_label = f"{indent}{drill_icon} {row['dimension_value']}"
    
                                def toggle_drill(path=row['path'], drilled=is_drilled):
                                    if drilled:
                                        if 'selected_drill_path' in st.session_state:
                                            del st.session_state['selected_drill_path']
                                    else:
                                        st.session_state['selected_drill_path'] = path
                                        st.session_state['drill_page'] = 0
    
                                drill_button_key = f"drill_{row['path']}"
                                st.button(
                                    button_label,
                                    key=drill_button_key,
                                    use_container_width=True,
                                    on_click=toggle_drill
                                )
    
                            # % of Total
                            with cols[2]:
                                pct_total = (row['queries'] / total_queries_all * 100) if total_queries_all > 0 else 0
                                st.write(f"{pct_total:.2f}%")
    
                            # Cumulative % (overall)
                            with cols[3]:
                                st.write(f"{row.get('cumulative_pct_overall', 0):.2f}%")
    
                            # % of Parent
                            with cols[4]:
                                st.write(f"{row['pct_of_parent']:.2f}%")
    
                            # Cumulative % (within parent)
                            with cols[5]:
                                st.write(f"{row['cumulative_pct_in_parent']:.2f}%")
    
                            # Dynamic metrics
                            for idx, metric_name in enumerate(metric_headers):
                                config = metric_config[metric_name]
                                with cols[num_base_cols + idx]:
                                    value = row[config['field']]
                                    st.write(config['format'](value))
    
                            # Show inline drill-down if this row is selected
                            if 'selected_drill_path' in st.session_state and st.session_state.selected_drill_path == row['path']:
                                st.divider()
    
                                # Parse the path to extract filters
                                path = row['path']
                                path_parts = path.split('/')
                                drill_filters = {}
    
                                # Build a map of paths to check for "Other" values
                                path_to_other_values = {}
                                current_path = ""
                                for part in path_parts:
                                    if '=' in part:
                                        current_path = f"{current_path}/{part}" if current_path else part
                                        dim_key, dim_value = part.split('=', 1)
                                        drill_filters[dim_key] = dim_value
    
                                        # Check if this path segment is an "Other" row
                                        matching_row = hierarchical_data[hierarchical_data['path'] == current_path]
                                        if not matching_row.empty and matching_row.iloc[0].get('is_other', False):
                                            path_to_other_values[dim_key] = matching_row.iloc[0].get('other_values', [])

                                # Get search terms data
                                if is_bigquery_mode:
                                    # Query BigQuery for search terms with dimension filters
                                    drill_query = dp.build_drill_down_query(
                                        st.session_state.bq_params['table'],
                                        base_filters,
                                        drill_filters,
                                        path_to_other_values if path_to_other_values else None
                                    )
                                    search_terms = dp.execute_cached_query(st.session_state.bq_client, drill_query, ttl=300)
                                else:
                                    # Filter the data from CSV
                                    drill_data = filtered_df.copy()
                                    for dim_key in drill_filters.keys():
                                        drill_data = dp.prepare_dimension_data(drill_data, dim_key)

                                    # Apply filters (handling "Other" rows specially)
                                    for dim_key, dim_value in drill_filters.items():
                                        if dim_key in path_to_other_values:
                                            # This is an "Other" row - filter by the list of values
                                            drill_data = drill_data[drill_data[dim_key].isin(path_to_other_values[dim_key])]
                                        else:
                                            # Regular filter
                                            drill_data = drill_data[drill_data[dim_key] == dim_value]

                                    # Aggregate by search term
                                    metric_cols = ['queries', 'queries_pdp', 'queries_a2c', 'purchases', 'gross_purchase']
                                    search_terms = drill_data.groupby('search_term')[metric_cols].sum().reset_index()
    
                                # Calculate rates
                                search_terms['ctr'] = np.where(search_terms['queries'] > 0,
                                                               search_terms['queries_pdp'] / search_terms['queries'], 0)
                                search_terms['a2c_rate'] = np.where(search_terms['queries'] > 0,
                                                                    search_terms['queries_a2c'] / search_terms['queries'], 0)
                                search_terms['conversion_rate'] = np.where(search_terms['queries'] > 0,
                                                                           search_terms['purchases'] / search_terms['queries'], 0)
                                search_terms['revenue_per_query'] = np.where(search_terms['queries'] > 0,
                                                                             search_terms['gross_purchase'] / search_terms['queries'], 0)
                                search_terms['avg_order_value'] = np.where(search_terms['purchases'] > 0,
                                                                           search_terms['gross_purchase'] / search_terms['purchases'], 0)
    
                                # Sort by queries descending
                                search_terms = search_terms.sort_values('queries', ascending=False)
    
                                # Apply minimum queries threshold - aggregate terms with fewer queries
                                terms_to_keep = search_terms[search_terms['queries'] >= min_queries_for_search_terms]
                                terms_to_aggregate = search_terms[search_terms['queries'] < min_queries_for_search_terms]
    
                                # If we have terms to aggregate, create "Other" row
                                if len(terms_to_aggregate) > 0:
                                    other_row = {
                                        'search_term': f'Other ({len(terms_to_aggregate)} terms)',
                                        'queries': terms_to_aggregate['queries'].sum(),
                                        'queries_pdp': terms_to_aggregate['queries_pdp'].sum(),
                                        'queries_a2c': terms_to_aggregate['queries_a2c'].sum(),
                                        'purchases': terms_to_aggregate['purchases'].sum(),
                                        'gross_purchase': terms_to_aggregate['gross_purchase'].sum(),
                                    }
                                    # Calculate rates for Other
                                    other_row['ctr'] = other_row['queries_pdp'] / other_row['queries'] if other_row['queries'] > 0 else 0
                                    other_row['a2c_rate'] = other_row['queries_a2c'] / other_row['queries'] if other_row['queries'] > 0 else 0
                                    other_row['conversion_rate'] = other_row['purchases'] / other_row['queries'] if other_row['queries'] > 0 else 0
                                    other_row['revenue_per_query'] = other_row['gross_purchase'] / other_row['queries'] if other_row['queries'] > 0 else 0
                                    other_row['avg_order_value'] = other_row['gross_purchase'] / other_row['purchases'] if other_row['purchases'] > 0 else 0
    
                                    # Append Other row to terms_to_keep
                                    import pandas as pd
                                    search_terms = pd.concat([terms_to_keep, pd.DataFrame([other_row])], ignore_index=True)
                                else:
                                    search_terms = terms_to_keep
    
                                # Pagination
                                page_size = 5
                                if 'drill_page' not in st.session_state:
                                    st.session_state.drill_page = 0
    
                                total_terms = len(search_terms)
                                total_pages = (total_terms + page_size - 1) // page_size
                                current_page = st.session_state.drill_page
    
                                start_idx = current_page * page_size
                                end_idx = min(start_idx + page_size, total_terms)
    
                                # Calculate cumulative percentages
                                total_queries_drill = search_terms['queries'].sum()
                                search_terms['cumulative_queries'] = search_terms['queries'].cumsum()
                                search_terms['cumulative_pct'] = (search_terms['cumulative_queries'] / total_queries_drill * 100) if total_queries_drill > 0 else 0
                                page_data = search_terms.iloc[start_idx:end_idx]
    
                                # Show page info and pagination
                                col_info, col_prev, col_next = st.columns([2, 1, 1])
                                with col_info:
                                    st.caption(f"üìÑ Showing {start_idx + 1}-{end_idx} of {total_terms} search terms (Page {current_page + 1}/{total_pages})")
                                with col_prev:
                                    if current_page > 0:
                                        if st.button("‚¨Ö Prev", key=f"prev_{row['path']}"):
                                            st.session_state.drill_page -= 1
                                            st.rerun()
                                with col_next:
                                    if current_page < total_pages - 1:
                                        if st.button("Next ‚û°", key=f"next_{row['path']}"):
                                            st.session_state.drill_page += 1
                                            st.rerun()
    
                                # Display search terms inline
                                for idx, term_row in page_data.iterrows():
                                    st_cols = st.columns(col_widths)
    
                                    # Empty expand button column
                                    with st_cols[0]:
                                        st.write("")
    
                                    # Search term with extra indent
                                    with st_cols[1]:
                                        extra_indent = "&nbsp;&nbsp;" * (row['level'] + 2)
                                        st.markdown(f"{extra_indent}üîç {term_row['search_term']}", unsafe_allow_html=True)
    
                                    # % Total
                                    with st_cols[2]:
                                        pct_total = (term_row['queries'] / total_queries_all * 100) if total_queries_all > 0 else 0
                                        st.write(f"{pct_total:.2f}%")
    
                                    # Cumulative % (overall) - not applicable for search terms
                                    with st_cols[3]:
                                        st.write("-")
    
                                    # % Parent (of the dimension row)
                                    with st_cols[4]:
                                        pct_parent = (term_row['queries'] / row['queries'] * 100) if row['queries'] > 0 else 0
                                        st.write(f"{pct_parent:.2f}%")
    
                                    # Cumulative % (within parent)
                                    with st_cols[5]:
                                        st.write(f"{term_row['cumulative_pct']:.2f}%")
    
                                    # Dynamic metrics
                                    for metric_idx, metric_name in enumerate(metric_headers):
                                        config = metric_config[metric_name]
                                        with st_cols[num_base_cols + metric_idx]:
                                            value = term_row[config['field']]
                                            st.write(config['format'](value))

                                st.divider()

                        # Add legend
                        st.caption("üí° **Tip**: Click ‚ûï to expand a row and see the next dimension breakdown. Click ‚ûñ to collapse.")
                        st.caption("üìä **Columns**: Cum%(O) = Cumulative % overall (level 0 only) | % Parent = % of parent row | Cum%(P) = Cumulative % within parent group")
                        st.caption("üîç **Click ‚ñ∂ on any dimension value** to see detailed search terms inline. Click üîΩ to collapse.")
                        st.caption("‚äï **Other (dimensions)**: Aggregates all dimension rows after the cumulative threshold is reached. Click ‚ñ∂ ‚äï to expand.")
                        st.caption(f"üîç **Other (search terms)**: Search terms with < {min_queries_for_search_terms} queries are aggregated together.")

                    else:
                        st.info("No visible rows. Try expanding some rows or adjusting filters.")
                else:
                    st.info("No data available for the selected dimensions with current filters.")
            except Exception as e:
                st.error(f"Error building table: {str(e)}")
                import traceback
                st.code(traceback.format_exc())

# Footer
st.divider()
st.caption(f"Dashboard last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Data range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}")
